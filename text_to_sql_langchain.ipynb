{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPL6jlJ1oCJ2QDQmgesgiso",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rrahul2203/SQLDatabaseQuerywithLLM/blob/main/text_to_sql_langchain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OBtj2YOhQD8V"
      },
      "outputs": [],
      "source": [
        "!pip install sqlalchemy==1.4.47\n",
        "!pip install snowflake-sqlalchemy\n",
        "!pip install langchain==0.0.166\n",
        "!pip install sqlalchemy-aurora-data-api\n",
        "!pip install PyAthena[SQLAlchemy]==2.25.2\n",
        "!pip install anthropic\n",
        "!pip install redshift-connector==2.0.910\n",
        "!pip install sqlalchemy-redshift==0.8.14"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import boto3\n",
        "\n",
        "import sqlalchemy\n",
        "from sqlalchemy import create_engine\n",
        "from snowflake.sqlalchemy import URL\n",
        "\n",
        "from langchain.docstore.document import Document\n",
        "from langchain import PromptTemplate,SagemakerEndpoint,SQLDatabase, SQLDatabaseChain, LLMChain\n",
        "from langchain.llms.sagemaker_endpoint import LLMContentHandler\n",
        "from langchain.chains.question_answering import load_qa_chain\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.chains import SQLDatabaseSequentialChain\n",
        "\n",
        "from langchain.chains.api.prompt import API_RESPONSE_PROMPT\n",
        "from langchain.chains import APIChain\n",
        "from langchain.prompts.prompt import PromptTemplate\n",
        "from langchain.chat_models import ChatAnthropic\n",
        "from langchain.chains.api import open_meteo_docs\n",
        "\n",
        "from typing import Dict\n",
        "\n",
        "CFN_STACK_NAME = \"cfn-genai-mda\"\n",
        "stacks = boto3.client('cloudformation').list_stacks()\n",
        "stack_found = CFN_STACK_NAME in [stack['StackName'] for stack in stacks['StackSummaries']]"
      ],
      "metadata": {
        "id": "XkONT2qmQV3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "def get_cfn_outputs(stackname: str) -> List:\n",
        "    cfn = boto3.client('cloudformation')\n",
        "    outputs = {}\n",
        "    for output in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Outputs']:\n",
        "        outputs[output['OutputKey']] = output['OutputValue']\n",
        "    return outputs\n",
        "\n",
        "def get_cfn_parameters(stackname: str) -> List:\n",
        "    cfn = boto3.client('cloudformation')\n",
        "    params = {}\n",
        "    for param in cfn.describe_stacks(StackName=stackname)['Stacks'][0]['Parameters']:\n",
        "        params[param['ParameterKey']] = param['ParameterValue']\n",
        "    return params\n",
        "\n",
        "if stack_found is True:\n",
        "    outputs = get_cfn_outputs(CFN_STACK_NAME)\n",
        "    params = get_cfn_parameters(CFN_STACK_NAME)\n",
        "    glue_crawler_name = params['CFNCrawlerName']\n",
        "    glue_database_name = params['CFNDatabaseName']\n",
        "    glue_databucket_name = params['DataBucketName']\n",
        "    region = outputs['Region']\n",
        "    print(f\"cfn outputs={outputs}\\nparams={params}\")\n",
        "else:\n",
        "    print(\"Recheck our cloudformation stack name\")"
      ],
      "metadata": {
        "id": "naE2gxsyTUmQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!aws s3 cp --recursive s3://covid19-lake/rearc-covid-19-testing-data/json/states_daily/ s3://test_example/covid-dataset/"
      ],
      "metadata": {
        "id": "uOGh9ZrJTWCi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python glue_crawler.py -c test_example"
      ],
      "metadata": {
        "id": "-UWh88nvTei6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "client = boto3.client('secretsmanager')\n",
        "anthropic_secret_id = \"anthropic\"\n",
        "response = client.get_secret_value(SecretId=anthropic_secret_id)\n",
        "secrets_credentials = json.loads(response['SecretString'])\n",
        "ANTHROPIC_API_KEY = secrets_credentials['ANTHROPIC_API_KEY']\n",
        "llm = ChatAnthropic(temperature=0, anthropic_api_key=ANTHROPIC_API_KEY, max_tokens_to_sample = 512)\n",
        "\n",
        "\n",
        "connathena=f\"athena.{region}.amazonaws.com\"\n",
        "portathena='443'\n",
        "schemaathena=glue_database_name\n",
        "s3stagingathena=f's3://{glue_databucket_name}/athenaresults/'\n",
        "wkgrpathena='primary'\n",
        "connection_string = f\"awsathena+rest://@{connathena}:{portathena}/{schemaathena}?s3_staging_dir={s3stagingathena}/&work_group={wkgrpathena}\"\n",
        "\n",
        "\n",
        "##  Create the athena  SQLAlchemy engine\n",
        "engine_athena = create_engine(connection_string, echo=False)\n",
        "dbathena = SQLDatabase(engine_athena)\n",
        "\n",
        "gdc = [schemaathena]"
      ],
      "metadata": {
        "id": "xERj2Ig0VBgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_catalog():\n",
        "    columns_str=''\n",
        "\n",
        "    #define glue cient\n",
        "    glue_client = boto3.client('glue')\n",
        "\n",
        "    for db in gdc:\n",
        "        response = glue_client.get_tables(DatabaseName =db)\n",
        "        for tables in response['TableList']:\n",
        "            if tables['StorageDescriptor']['Location'].startswith('s3'):  classification='s3'\n",
        "            else:  classification = tables['Parameters']['classification']\n",
        "            for columns in tables['StorageDescriptor']['Columns']:\n",
        "                    dbname,tblname,colname=tables['DatabaseName'],tables['Name'],columns['Name']\n",
        "                    columns_str=columns_str+f'\\n{classification}|{dbname}|{tblname}|{colname}'\n",
        "\n",
        "    columns_str=columns_str+'\\n'+('api|meteo|weather|weather')\n",
        "    return columns_str\n",
        "\n",
        "glue_catalog = parse_catalog()\n",
        "\n",
        "#display a few lines from the catalog\n",
        "print('\\n'.join(glue_catalog.splitlines()[-10:]) )"
      ],
      "metadata": {
        "id": "NkXOJ8T7Vkjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def identify_channel(query):\n",
        "    prompt_template = \"\"\"\n",
        "     From the table below, find the database (in column database) which will contain the data (in corresponding column_names) to answer the question\n",
        "     {query} \\n\n",
        "     \"\"\"+glue_catalog +\"\"\"\n",
        "     Give your answer as database ==\n",
        "     Also,give your answer as database.table ==\n",
        "     \"\"\"\n",
        "    ##define prompt 1\n",
        "    PROMPT_channel = PromptTemplate( template=prompt_template, input_variables=[\"query\"]  )\n",
        "\n",
        "    # define llm chain\n",
        "    llm_chain = LLMChain(prompt=PROMPT_channel, llm=llm)\n",
        "    #run the query and save to generated texts\n",
        "    generated_texts = llm_chain.run(query)\n",
        "    print(generated_texts)\n",
        "\n",
        "    #set the best channel from where the query can be answered\n",
        "    if 's3' in generated_texts:\n",
        "        channel='db'\n",
        "        db=dbathena\n",
        "        print(\"SET database to athena\")\n",
        "    elif 'api' in generated_texts:\n",
        "        channel='api'\n",
        "        print(\"SET database to weather api\")\n",
        "    else:\n",
        "        raise Exception(\"User question cannot be answered by any of the channels mentioned in the catalog\")\n",
        "\n",
        "    print(\"Step complete. Channel is: \", channel)\n",
        "\n",
        "    return channel, db"
      ],
      "metadata": {
        "id": "x06Gp_eyVwIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_query(query):\n",
        "    channel, db = identify_channel(query)\n",
        "\n",
        "    _DEFAULT_TEMPLATE = \"\"\"Given an input question, first create a syntactically correct {dialect} query to run, then look at the results of the query and return the answer.\n",
        "\n",
        "    Do not append 'Query:' to SQLQuery.\n",
        "\n",
        "    Display SQLResult after the query is run in plain english that users can understand.\n",
        "\n",
        "    Provide answer in simple english statement.\n",
        "\n",
        "    Only use the following tables:\n",
        "\n",
        "    {table_info}\n",
        "    If someone asks for the sales, they really mean the tickit.sales table.\n",
        "    If someone asks for the sales date, they really mean the column tickit.sales.saletime.\n",
        "\n",
        "    Question: {input}\"\"\"\n",
        "\n",
        "    PROMPT_sql = PromptTemplate(\n",
        "        input_variables=[\"input\", \"table_info\", \"dialect\"], template=_DEFAULT_TEMPLATE\n",
        "    )\n",
        "\n",
        "\n",
        "    if channel=='db':\n",
        "        db_chain = SQLDatabaseChain.from_llm(llm, db, prompt=PROMPT_sql, verbose=True, return_intermediate_steps=False)\n",
        "        response=db_chain.run(query)\n",
        "    elif channel=='api':\n",
        "        chain_api = APIChain.from_llm_and_api_docs(llm, open_meteo_docs.OPEN_METEO_DOCS, verbose=True)\n",
        "        response=chain_api.run(query)\n",
        "    else: raise Exception(\"Unlisted channel. Check your unified catalog\")\n",
        "    return response"
      ],
      "metadata": {
        "id": "iaAAVay7WVzj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"\"\"Which States reported the least and maximum deaths?\"\"\"\n",
        "\n",
        "#Response from Langchain\n",
        "response =  run_query(query)\n",
        "print(f'SQL and response from user query {query}  \\n  {response}')"
      ],
      "metadata": {
        "id": "zCbQ3HtPWtaN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}